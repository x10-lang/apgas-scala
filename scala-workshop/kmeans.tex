\section{Distributed \kmeans Clustering}
\label{sec:kmeans}

The \kmeans benchmark uses Lloyd's algorithm~\cite{Lloyd1982Least} to divide a
set of points in a $d$-dimensional space into $k$ disjoint clusters.  Given an
arbitrary set of initial clusters, the algorithm iterates over the following
steps:
\begin{enumerate}
  \item For each point, assign that point to whichever cluster is closest (by
Euclidean distance to the cluster centroid).
  \item For each cluster, update the centroid (the arithmetic mean of all
points assigned to that cluster).
\end{enumerate}
Distributed computation is straightforward: each process holds a portion of the
points and computes cluster assignments and centroid contributions for each
point. At each iteration, a master process collects all centroid contributions,
computes the aggregates, checks if the computation has converged, and if not,
communicates the updated values to all workers.

Figure~\ref{fig:kmeansapgas} shows the main structure of a distributed \kmeans
computation with \apgas. The state is split between the master's view of 1) the
centroids and 2) the contributions being collected, and the workers'
place-local memory, comprising a subset of points and the local view of the
centroids. The place-local memory is held in \lstinline{local}, of type
\lstinline{GlobalRef[LocalData]}.

The structure of the computation, including the distribution
aspect, is fully explicit in the code: the outermost \lstinline{while} loop
iterates until convergence, the \lstinline{for} loop spawns an activity to be run asynchronously at each place as indicated by \lstinline{asyncAt}, which in turn spawns a remote activity at the master place to combine the place's local view with the master's view.
Finally, \lstinline{finish} ensures that all remote work has completed before
proceeding to the next iteration.
\begin{figure}
\begin{lstlisting}
class ClusterState extends Serializable {
  val centroids $\EQ$ Array.ofDim[Float](K, D)
  val counts $\EQ$ Array.ofDim[Int](K)
}
class LocalData(val points: ..., val state: ClusterState) { ... }
val local: LocalData $\EQ$ GlobalRef.forPlaces(places) { ... }
val masterState $\EQ$ new ClusterState()
val masterRef $\EQ$ SharedRef.make(masterState)
val currentCentroids $\EQ$ Array.ofDim[Float](K, D)
while (!converged()) {
  finish {
    reset(newCentroids); reset(newCounts)
    for (p $\lA$ places) {
      asyncAt(place) {
        val pState = local().state
        val points = local().points
        compute(currentCentroids, points, pState)
        asyncAt(masterRef.home) {
          val masterCentroids $\EQ$ masterRef().centroids
          masterCentroids.synchronized {
            ... /* add elements from pState.centroids */ }
          val masterCounts $\EQ$ masterRef().counts
          masterCounts.synchronized {
            ... /* add elements from pState.counts */ }
        }
      }
  } } }
  ... // normalize centroids by counts
  copyArray(masterState.centroids, currentCentroids)
}
\end{lstlisting}
\caption{Code structure for \kmeans in \apgas.\label{fig:kmeansapgas}}
\end{figure}
An aspect of the code that can be harder to grasp is the movement of data: the
value of \lstinline{currentCentroids} is sent from the master to a worker by letting the variable be captured in the closure passed to \lstinline{asyncAt}.
Note that while \lstinline{local} is a
\lstinline{GlobalRef} and is therefore never serialized implicitly, we use
\lstinline{apply} to dereference it and thus pass a copy of the data of type
\lstinline{LocalData} to the master process in the nested \lstinline{asyncAt}.
Finally, note that the code that adds the contribution of a worker to the master values is synchronized to avoid data races.

For contrast, Figure~\ref{fig:kmeansakka} shows the related parts of an
actor-based implementation of \kmeans clustering using Akka.
\begin{figure}
\begin{lstlisting}
  class Master(...) extends Actor {
    val workers: Seq[ActorRef] $\EQ$ ...
    val centroids, newCentroids $\EQ$ Array.ofDim[Float](K, D)
    val newCounts $\EQ$ Array.ofDim[Int](K)
    var received $\EQ$ 0
    override def receive $\EQ$ {
      case Run $\RA$ if(!converged()) {
        reset(newCentroids); reset(newCounts)
        received $\EQ$ 0
        workers.foreach(_ ! Update(centroids)) }
      case Updated(workerCentroids, workerCounts) $\RA$
        ... /* add elements from pState.centroids */ }
        ... /* add elements from pState.counts */ }
        received $\PEQ$ 1
        if(received $\EQEQ$ numWorkers) {
          ... // normalize newCentroids by newCounts
          copyArray(newCentroids, centroids)
          self ! Run }
  } }
  class Worker(...) extends Actor {
    val points $\EQ$ ...
    val localCentroids $\EQ$ ...; val localCounts $\EQ$ ...
    override def receive $\EQ$ {
      case Update(centroids) $\RA$
        compute(centroids, this, ...)
        sender ! Updated(localCentroids, localCounts)
  } }
\end{lstlisting}
\caption{Code structure for \kmeans in Akka.\label{fig:kmeansakka}}
\end{figure}
Almost as a dual to the \apgas implementation, the movement of data is entirely
explicit, but the control flow must be inferred from the flow of messages: the
master actor sends itself \lstinline{Run} messages to continue the computation,
and must keep count of how many \lstinline{Updated} messages it received from
workers to know when an iteration is complete. There is no need for data
synchronization, as the model enforces that message processing within an actor
is always a sequential operation.
