\section{Distributed \kmeans Clustering}
\label{sec:kmeans}

The \kmeans benchmark uses Lloyd's algorithm~\cite{Lloyd1982Least} to divide a
set of points in a $d$-dimensional space into $k$ disjoint clusters.  Given an
arbitrary set of initial clusters, the algorithm iterates over the following
steps:
\begin{enumerate}
  \item For each point, assign that point to whichever cluster is closest (by
Euclidean distance to the cluster centroid).
  \item For each cluster, update the centroid (the arithmetic mean of all
points assigned to that cluster).
\end{enumerate}
Distributed computation is straightforward: each process holds a portion of the
points and computes cluster assignments and centroid contributions for each
point. At each iteration, a master process collects all centroid contributions,
computes the aggregates, checks if the computation has converged, and if not,
communicates the updated values to all workers.

Figure~\ref{fig:kmeansapgas} shows the main structure of a distributed \kmeans
computation with \apgas. The state is split between the master's view of 1) the
centroids and 2) the contributions being collected, and the workers'
place-local memory, comprising a subset of points and the local view of the
centroids. The place-local memory is held in \lstinline{local}, of type
\lstinline{GlobalRef[LocalData]}.

The structure of the computation, including the distribution
aspect, is fully explicit in the code: the outermost \lstinline{while} loop
iterates until convergence, The \lstinline{for} loop includes work related to
each place, to be run asynchronously, as indicated by \lstinline{async}, and 
a part of that work is to be executed remotely, as indicated by \lstinline{at}.
Finally, \lstinline{finish} ensures that all remote work has completed before
proceeding to the next iteration.
\begin{figure}
\begin{lstlisting}
class LocalData(numPoints: Int) extends Serializable {
  val points    $\EQ$ Array.ofDim[Float](numPoints, D)
  val centroids $\EQ$ Array.ofDim[Float](K, D)
  val counts    $\EQ$ Array.ofDim[Int](K)
}
val local $\EQ$ GlobalRef.forPlaces(places) { $\ldots$ }

val centroids, newCentroids $\EQ$ Array.ofDim[Float](K, D)
val newCounts $\EQ$ Array.ofDim[Int](K)

while (!converged()) {
  finish {
    reset(newCentroids); reset(newCounts)
    for (p $\lA$ places) {
      async {
        val pState: LocalData $\EQ$ at(p) {
          compute(centroids, local, $\ldots$)
          local()
        }
        newCentroids.synchronized {
          $\ldots$ /* add elements from pState.centroids */ }
        newCounts.synchronized {
          $\ldots$ /* add elements from pState.counts */ }
  } } }
  $\ldots$ // normalize newCentroids by newCounts
  copyArray(newCentroids, centroids)
}
\end{lstlisting}
\caption{Code structure for \kmeans in \apgas.\label{fig:kmeansapgas}}
\end{figure}
An aspect of the code that can be harder to grasp is the movement of data: the
value of \lstinline{centroids} is sent from the master to a worker by letting
the variable be captured in the closure passed to \lstinline{at}. The other
direction is more explicit: \lstinline{at} blocks until the value
\lstinline{local()} is returned. Note that while \lstinline{local} is a
\lstinline{GlobalRef} and is therefore never serialized implicitly, here we use
\lstinline{apply} to dereference it and thus pass a copy of the data of type
\lstinline{LocalData} to the master process. Finally, note that the portion of
the code that adds the contribution of a worker to the master values must be
protected for data races.

For contrast, Figure~\ref{fig:kmeansakka} shows the related parts of an
actor-based implementation of \kmeans clustering using Akka.
\begin{figure}
\begin{lstlisting}
  class Master($\ldots$) extends Actor {
    val workers: Seq[ActorRef] $\EQ$ $\ldots$
    val centroids, newCentroids $\EQ$ Array.ofDim[Float](K, D)
    val newCounts $\EQ$ Array.ofDim[Int](K)
    var received $\EQ$ 0
    override def receive $\EQ$ {
      case Run $\RA$ if(!converged()) {
        reset(newCentroids); reset(newCounts)
        received $\EQ$ 0
        workers.foreach(_ ! Update(centroids)) }
      case Updated(workerCentroids, workerCounts) $\RA$
        $\ldots$ /* add elements from pState.centroids */ }
        $\ldots$ /* add elements from pState.counts */ }
        received $\PEQ$ 1
        if(received $\EQEQ$ numWorkers) {
          $\ldots$ // normalize newCentroids by newCounts
          copyArray(newCentroids, centroids)
          self ! Run }
  } }
  class Worker($\ldots$) extends Actor {
    val points $\EQ$ $\ldots$
    val localCentroids $\EQ$ $\ldots$; val localCounts $\EQ$ $\ldots$
    override def receive $\EQ$ {
      case Update(centroids) $\RA$
        compute(centroids, this, $\ldots$)
        sender ! Updated(localCentroids, localCounts)
  } }
\end{lstlisting}
\caption{Code structure for \kmeans in Akka.\label{fig:kmeansakka}}
\end{figure}
Almost as a dual to the \apgas implementation, the movement of data is entirely
explicit, but the control flow must be inferred from the flow of messages: the
master actor sends itself \lstinline{Run} messages to continue the computation,
and must keep count of how many \lstinline{Updated} messages it received from
workers to know when an iteration is complete. There is no need for data
synchronization, as the model enforces that message processing within an actor
is always a sequential operation.
