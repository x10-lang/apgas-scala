\section{Performance Evaluation}
\label{sec:perf}
We ran our \apgas and Akka implementations of \kmeans and UTS on a 48 core
machine, measuring the performance of configurations with 1, 2, 4, 8, 16, and
32 workers. For the \apgas programs, the number of workers corresponds to the
number of places. For the Akka programs, $n$ workers correspond to $n+1$
actors: both benchmarks use the idiom of a master actor supervising the
workers and detecting termination, as described in Sections~\ref{sec:kmeans}
and~\ref{sec:uts}. Because we are primarily interested in the scaling profile of
our applications, we normalize the performance by the number of workers.

We ran our Akka programs by allocating one process for each worker actor, and
using \lstinline{akka-remote} for communication. This configuration is close to
\apgas in terms of communication constraints,\footnote{Places in \apgas are
currently only realized as separate processes.} and we believe it reflects
typical distributed computing applications. All numbers were obtained by
averaging the results of three runs.

For \kmeans, we fixed the problem input size to $32$ million $4$-dimensional
points and $5$ centroids, and measured performance as the number of iterations
per second. The core computational code (determining the closest centroid for
each point) is common to the benchmarks. Figure~\ref{fig:kmeans-scaling} shows
the effect of scaling the number of workers for the \apgas and Akka
implementations (note the tight scale). The scaling profiles are overall
similar, with an initial improvement in per-worker throughput, possibly due to
increased available memory bandwidth when using multiple sockets.
The performance advantage of the \apgas implementation can be explained by how merging of partial results is handled:
the corresponding two arrays are locked individually, whereas in the Akka
implementation, the messages carrying the partial results are processed
sequentially.

\begin{figure}
\vspace{-0.3cm}
\hspace{-0.2cm}
\begingroup\graphicspath{{figures/}}\input{figures/kmeans}\endgroup
\vspace{-0.2cm}
\caption{Scaling of \kmeans implementations.}
\label{fig:kmeans-scaling}
\end{figure}

For UTS, we measured the rate of traversal of a tree of depth 15, in millions
of nodes per second. Most of the computational work is hashing, for which the
code is shared. Figure~\ref{fig:uts-scaling} shows that the scaling profiles
are very similar for the two implementations. 

\begin{figure}
\vspace{-0.3cm}
\hspace{-0.2cm}
\begingroup\graphicspath{{figures/}}\input{figures/uts}\endgroup
\vspace{-0.2cm}
\caption{Scaling of UTS implementations.}
\label{fig:uts-scaling}
\end{figure}


% \paragraph{UTS.} We measured the rate of traversal of our Akka and
% \apgas implementations of UTS in millions of nodes per second (Mn/s), using
% 32-way parallelism on a 48 core machine. We measured three configurations: 1)
% \apgas implementation on 32 processes, 2) Akka implementation on 1 process with
% 32 worker actors, 3) Akka implementation on 32 processes, each with 1 worker
% actor.\footnote{As places in \apgas are currently only realized as separate
% processes, configuration 3) is closer in terms of communication constraints.}
% We used \lstinline{akka-remote} for 3), and the flexibility of the actor model
% means that the distribution of workers per process is isolated to a single
% invocation of \lstinline{.withDeploy}. The configurations achieved 269.4Mn/s,
% 286.8Mn/s, and 274.2Mn/s, respectively. This shows that the \apgas code comes
% within 98\% and 93\% of the performance of the multi-process and single-process
% Akka configurations, respectively.

